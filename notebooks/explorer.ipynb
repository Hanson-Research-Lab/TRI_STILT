{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python37764bitstiltvenvd0b5b9a6265a460294244206ccd6e7b1",
   "display_name": "Python 3.7.7 64-bit ('stilt': venv)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Basic py: \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import datetime\n",
    "import os\n",
    "import math\n",
    "import json\n",
    "from urllib.request import urlopen\n",
    "import glob\n",
    "#Vis\n",
    "#import seaborn as sns\n",
    "\n",
    "#Geo\n",
    "import geopandas as gpd\n",
    "import fiona\n",
    "from shapely.geometry import Point\n",
    "import descartes\n",
    "import contextily as ctx #Basemaps \n",
    "import xarray as xr "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preperation to get TRI data into STILT ready format: \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load TRI dat: \n",
    "tri_df = pd.read_csv('/home/boogie2/Hanson_Lab/TRI_STILT/data/processed/TRI_base_process_90_99.csv').drop(columns=['Unnamed: 0'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This seperates fugitive and stack releases - setting the stack height of the release for fugitive releases to 0\n",
    "fug = tri_df[tri_df['51-FUGITIVEAIR']>0]\n",
    "fug['StackHeight']=0\n",
    "fug = fug.rename(columns = {'51-FUGITIVEAIR':'Release (lbs/year)'})\n",
    "fug = fug.drop(columns = ['52-STACKAIR'])\n",
    "\n",
    "stack = tri_df[tri_df['52-STACKAIR']>0]\n",
    "stack = stack.rename(columns = {'52-STACKAIR':'Release (lbs/year)'})\n",
    "stack = stack.drop(columns = ['51-FUGITIVEAIR'])\n",
    "\n",
    "stack_fug_df = pd.concat([stack,fug])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "0      30.5\n1      10.7\n2      10.7\n3      10.7\n4      10.7\n       ... \n895     0.0\n896     0.0\n897     0.0\n898     0.0\n899     0.0\nName: StackHeight, Length: 1379, dtype: float64"
     },
     "metadata": {},
     "execution_count": 100
    }
   ],
   "source": [
    "stack_fug_df['StackHeight']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "def STILT_converter(df,min_year,max_year,save_base_name):\n",
    "    \"\"\"\n",
    "    Converts a dataframe containing LATITUDE LONGITUDE Stackheight, CHEMICAL and a zagl to a csv for STILT pipeline. \n",
    "\n",
    "    ===\n",
    "    Inputs:\n",
    "    1. df - dataframe containing expanded fugitive and stack releases, renamed zagl. \n",
    "    2. min_year - minimum year of TRI releases \n",
    "    3. max_year - maximum year of analysis for TRI releases\n",
    "    4. save_base_name - the base save name for an id mappings file (USE FOR JOIN after STILT RUN) and a stilt run (USE FOR STILT) csv file\n",
    "\n",
    "    Outputs: \n",
    "    1. Saves all files - no output returned \n",
    "    \"\"\"\n",
    "\n",
    "    #Create a base dataframe which houses all stilt runs as seperated by lat/long/stackheight/chemical/amount and year. \n",
    "    base_df = df[(df.YEAR >= min_year) & (df.YEAR <=max_year)][['LATITUDE','LONGITUDE','StackHeight','CHEMICAL','Release (lbs/year)','YEAR']].rename(columns={'LATITUDE':'lati','LONGITUDE':'long','CHEMICAL':'Chemical','StackHeight':'zagl'})\n",
    "\n",
    "    #Stilt only runs particles based upon location and time (concentration unnessecary. Create a subset to run simulations on and an id to remerge on (for convolutional toxicity calculation))\n",
    "    stilt_run_id = base_df.drop_duplicates(['lati','long','zagl','YEAR']).drop(columns=['Chemical','Release (lbs/year)']).sort_values(by='YEAR').reset_index(drop=True).reset_index().rename(columns = {'index':'id'})\n",
    "\n",
    "    #Add the id to the base_df \n",
    "    stilt_trace_mapping = base_df.merge(stilt_run_id, on=['lati','long','zagl','YEAR']).sort_values(by='id')\n",
    "\n",
    "    #save the files\n",
    "    stilt_trace_mapping.to_csv(str(save_base_name + '_id_mappings.csv'),index=False)\n",
    "    stilt_run_id.to_csv(str(save_base_name + '_stilt_RUN.csv'),index = False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "STILT_converter(stack_fug_df,1990,1999,'/home/boogie2/Hanson_Lab/TRI_STILT/data/processed/unique_TRI_location_height_year')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}